{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EpT8OdWoo-Hi"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yXSHZB2o-Hk",
    "outputId": "c179d22e-6260-4e24-9761-0befa30a0543"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('students', 0.7294867038726807),\n",
       " ('Student', 0.6706662774085999),\n",
       " ('teacher', 0.6301366090774536),\n",
       " ('stu_dent', 0.6240993142127991),\n",
       " ('faculty', 0.6087332963943481),\n",
       " ('school', 0.6055627465248108),\n",
       " ('undergraduate', 0.6020305752754211),\n",
       " ('university', 0.600540041923523),\n",
       " ('undergraduates', 0.5755698680877686),\n",
       " ('semester', 0.573759913444519)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1.1 (\"student\")\n",
    "google_news_vectors.most_similar(\"student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qEdHwSiLo-Hk",
    "outputId": "efd70677-7f41-4d4f-f4c4-dca97ff53c55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple_AAPL', 0.7456986308097839),\n",
       " ('Apple_Nasdaq_AAPL', 0.7300410270690918),\n",
       " ('Apple_NASDAQ_AAPL', 0.717508852481842),\n",
       " ('Apple_Computer', 0.7145972847938538),\n",
       " ('iPhone', 0.6924266219139099),\n",
       " ('Apple_NSDQ_AAPL', 0.6868603229522705),\n",
       " ('Steve_Jobs', 0.6758421659469604),\n",
       " ('iPad', 0.6580768823623657),\n",
       " ('Apple_nasdaq_AAPL', 0.6444970369338989),\n",
       " ('AAPL_PriceWatch_Alert', 0.6439753174781799)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1.1 (\"Apple\")\n",
    "google_news_vectors.most_similar(\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqvbJ2Jlo-Hl",
    "outputId": "2e968297-de37-46aa-cc5b-75e5522a3787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302295327186584),\n",
       " ('pears', 0.613396167755127),\n",
       " ('strawberry', 0.6058260798454285),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935863852500916),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1.1 (\"apple\")\n",
    "google_news_vectors.most_similar(\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C04Y8TOto-Hl",
    "outputId": "d6b70061-71ea-416b-88ed-823fc61948b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC', 'O', 'I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-MISC'} \n",
      "\n",
      "Number of Sentences in Train file:  14041\n"
     ]
    }
   ],
   "source": [
    "# Question 1.2 Part a (Train file)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file = open('eng.train', 'r')\n",
    "train_file_lines = train_file.readlines()\n",
    "\n",
    "count = 0\n",
    "labels_set = set()\n",
    "docstart_idx = 0\n",
    "word_list = {}\n",
    "sentences_two_entities = {}\n",
    "labels_in_sentence = set()\n",
    "train_df = pd.DataFrame()\n",
    "all_word_list_train = []\n",
    "all_word_labels_train = []\n",
    "for idx, line in enumerate(train_file_lines):\n",
    "    line_list = line.split()\n",
    "    if len(line_list) != 0 and not line.startswith(\"-DOCSTART-\"):\n",
    "        word = line_list[0]\n",
    "        label = line_list[-1]\n",
    "        word_list[word] = label\n",
    "        labels_set.add(label)\n",
    "        labels_in_sentence.add(label)\n",
    "        all_word_list_train.append(word)\n",
    "        all_word_labels_train.append(label)\n",
    "    if line.startswith(\"-DOCSTART-\"):\n",
    "        docstart_idx = idx\n",
    "        continue\n",
    "    if len(line_list) == 0 and train_file_lines[idx-1].startswith(\"-DOCSTART-\"):\n",
    "        continue\n",
    "    elif len(line_list) == 0:\n",
    "        count += 1\n",
    "        if len(labels_in_sentence - {'O'}) >= 2 or len(labels_in_sentence) >= 3:\n",
    "            full_sentence = ' '.join(list(word_list.keys()))\n",
    "            sentence = \"\"\n",
    "            for word, label in word_list.items():\n",
    "                sentence = sentence + word + \" [\" + label + \"] \"\n",
    "            sentences_two_entities[sentence] = [full_sentence, word_list]\n",
    "        word_list = {}\n",
    "        labels_in_sentence = set()\n",
    "\n",
    "print(labels_set, \"\\n\")\n",
    "print(\"Number of Sentences in Train file: \", count)\n",
    "\n",
    "train_df['word'] = all_word_list_train\n",
    "train_df['label'] = all_word_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FH4qZ3Lco-Hm",
    "outputId": "a1070153-54de-4401-f841-5ca0ac99ea84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC', 'O', 'I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-MISC'} \n",
      "\n",
      "Number of Sentences in Dev file:  3453\n"
     ]
    }
   ],
   "source": [
    "# Question 1.2 Part a (Dev file)\n",
    "\n",
    "dev_file = open('eng.testa', 'r')\n",
    "dev_file_lines = dev_file.readlines()\n",
    "\n",
    "count = 0\n",
    "# labels_set = set()\n",
    "docstart_idx = 0\n",
    "dev_df = pd.DataFrame()\n",
    "all_word_list_dev = []\n",
    "all_word_labels_dev = []\n",
    "for idx, line in enumerate(dev_file_lines):\n",
    "    line_list = line.split()\n",
    "    if len(line_list) != 0 and not line.startswith(\"-DOCSTART-\"):\n",
    "        word = line_list[0]\n",
    "        label = line_list[-1]\n",
    "        labels_set.add(label)\n",
    "        all_word_list_dev.append(word)\n",
    "        all_word_labels_dev.append(label)\n",
    "    if line.startswith(\"-DOCSTART-\"):\n",
    "        docstart_idx = idx\n",
    "        continue\n",
    "    if len(line_list) == 0 and train_file_lines[idx-1].startswith(\"-DOCSTART-\"):\n",
    "        continue\n",
    "    elif len(line_list) == 0:\n",
    "        count += 1\n",
    "\n",
    "print(labels_set, \"\\n\")\n",
    "print(\"Number of Sentences in Dev file: \", count)\n",
    "\n",
    "dev_df['word'] = all_word_list_dev\n",
    "dev_df['label'] = all_word_labels_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDlu6At_o-Hn",
    "outputId": "88ad5a6a-054a-44f8-cdb6-73e98d66b8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC', 'O', 'I-PER', 'I-ORG', 'I-LOC', 'B-LOC', 'B-ORG', 'B-MISC'} \n",
      "\n",
      "Number of Sentences in Test file:  3667\n"
     ]
    }
   ],
   "source": [
    "# Question 1.2 Part a (Test file)\n",
    "\n",
    "test_file = open('eng.testb', 'r')\n",
    "test_file_lines = test_file.readlines()\n",
    "\n",
    "count = 0\n",
    "# labels_set = set()\n",
    "docstart_idx = 0\n",
    "test_df = pd.DataFrame()\n",
    "all_word_list_test = []\n",
    "all_word_labels_test = []\n",
    "for idx, line in enumerate(test_file_lines):\n",
    "    line_list = line.split()\n",
    "    if len(line_list) != 0 and not line.startswith(\"-DOCSTART-\"):\n",
    "        word = line_list[0]\n",
    "        label = line_list[-1]\n",
    "        labels_set.add(label)\n",
    "        all_word_list_test.append(word)\n",
    "        all_word_labels_test.append(label)\n",
    "    if line.startswith(\"-DOCSTART-\"):\n",
    "        docstart_idx = idx\n",
    "        continue\n",
    "    if len(line_list) == 0 and train_file_lines[idx-1].startswith(\"-DOCSTART-\"):\n",
    "        continue\n",
    "    elif len(line_list) == 0:\n",
    "        count += 1\n",
    "\n",
    "print(labels_set, \"\\n\")\n",
    "print(\"Number of Sentences in Test file: \", count)\n",
    "\n",
    "test_df['word'] = all_word_list_test\n",
    "test_df['label'] = all_word_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RfdMp6bo-Hn",
    "outputId": "0373185f-35da-4c66-b8ec-396ddbb1b654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The [O] SPLA [I-ORG] has [O] fought [O] Khartoum [I-LOC] 's [O] government [O] forces [O] in [O] the [O] south [O] since [O] 1983 [O] for [O] greater [O] autonomy [O] or [O] independence [O] of [O] mainly [O] Christian [I-MISC] and [O] animist [O] region [O] from [O] Moslem [I-MISC] , [O] Arabised [I-LOC] north [I-LOC] . [O]  \n",
      "\n",
      "The SPLA has fought Khartoum 's government forces in the south since 1983 for greater autonomy or independence of mainly Christian and animist region from Moslem , Arabised north . \n",
      "\n",
      "{'The': 'O', 'SPLA': 'I-ORG', 'has': 'O', 'fought': 'O', 'Khartoum': 'I-LOC', \"'s\": 'O', 'government': 'O', 'forces': 'O', 'in': 'O', 'the': 'O', 'south': 'O', 'since': 'O', '1983': 'O', 'for': 'O', 'greater': 'O', 'autonomy': 'O', 'or': 'O', 'independence': 'O', 'of': 'O', 'mainly': 'O', 'Christian': 'I-MISC', 'and': 'O', 'animist': 'O', 'region': 'O', 'from': 'O', 'Moslem': 'I-MISC', ',': 'O', 'Arabised': 'I-LOC', 'north': 'I-LOC', '.': 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Question 1.2 Part b\n",
    "random_sentence = random.choice(list(sentences_two_entities.keys()))\n",
    "original_sentence = sentences_two_entities[random_sentence][0]\n",
    "word_list_labels = sentences_two_entities[random_sentence][1]\n",
    "print(random_sentence, \"\\n\")\n",
    "print(original_sentence, \"\\n\")\n",
    "print(word_list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ftwu6Ub3xNh_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 07:46:30.379123: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-10 07:46:30.381184: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-10 07:46:30.397360: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-10 07:46:30.397376: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-10 07:46:30.397391: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 07:46:30.400876: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-10 07:46:30.401252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 07:46:30.875947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Extract words and NER labels from dataframes\n",
    "train_words = train_df['word'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "dev_words = dev_df['word'].tolist()\n",
    "dev_labels = dev_df['label'].tolist()\n",
    "test_words = test_df['word'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "\n",
    "# Tokenize and encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "dev_labels_encoded = label_encoder.transform(dev_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(list(google_news_vectors.index_to_key))\n",
    "embedding_dim = 300  # Match the dimension of the Word2Vec embeddings\n",
    "hidden_units = 64\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9heSr3ipjiq1"
   },
   "outputs": [],
   "source": [
    "# Create sequences of word indices\n",
    "train_word_indices = [google_news_vectors.key_to_index[word] if word in google_news_vectors.key_to_index else 0 for word in train_words]\n",
    "dev_word_indices = [google_news_vectors.key_to_index[word] if word in google_news_vectors.key_to_index else 0 for word in dev_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "q2w22gbpgAWQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 07:46:36.411452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-10 07:46:36.470781: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sh1uTeZ6mG6N"
   },
   "outputs": [],
   "source": [
    "# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, trainable=False, weights=[google_news_vectors.vectors]))\n",
    "# embedding_layer = Embedding(\n",
    "#     input_dim=vocab_size,\n",
    "#     output_dim=embedding_dim,\n",
    "#     input_length=1,\n",
    "#     trainable=False,\n",
    "#     weights=[np.array([google_news_vectors[word] if word in list(google_news_vectors.index_to_key) else np.zeros(embedding_dim) for word in google_news_vectors.index_to_key])]\n",
    "# )\n",
    "# model.add(embedding_layer)\n",
    "\n",
    "# def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "#     \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     train_embeddings : bool\n",
    "#         If False, the returned weights are frozen and stopped from being updated.\n",
    "#         If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     `keras.layers.Embedding`\n",
    "#         Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "#     \"\"\"\n",
    "#     keyed_vectors = model  # structure holding the result of training\n",
    "#     # weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array\n",
    "#     weights = np.vstack([keyed_vectors[word] for word in list(keyed_vectors.index_to_key)])  # vectors themselves, a 2D numpy array\n",
    "#     index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "#     layer = Embedding(\n",
    "#         input_dim=weights.shape[0],\n",
    "#         output_dim=weights.shape[1],\n",
    "#         weights=[weights],\n",
    "#         trainable=train_embeddings,\n",
    "#     )\n",
    "#     return layer\n",
    "\n",
    "# model.add(gensim_to_keras_embedding(google_news_vectors))\n",
    "\n",
    "train_unique_words = set(train_words)\n",
    "dev_unique_words = set(dev_words)\n",
    "test_unique_words = set(test_words)\n",
    "all_unique_words = train_unique_words | dev_unique_words | test_unique_words\n",
    "\n",
    "embedding_weights = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# for word in all_unique_words:\n",
    "#     if word in google_news_vectors.key_to_index:\n",
    "#         index = google_news_vectors.key_to_index[word]\n",
    "#         embedding_weights[index] = google_news_vectors[word]\n",
    "\n",
    "for word, index in google_news_vectors.key_to_index.items():\n",
    "    embedding_weights[index] = google_news_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f1cYFU4SglJv"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=1,\n",
    "    weights=[embedding_weights],\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JMAzUaW7mKTj"
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(units=hidden_units, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "7jbDunk4mMe4"
   },
   "outputs": [],
   "source": [
    "# model.add(TimeDistributed(Dense(num_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0kWCDD3HmOBF"
   },
   "outputs": [],
   "source": [
    "# model.add(Activation('softmax'))\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Wj-E1MpFmSoK"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "X3CpatQCyxb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1606/1606 [==============================] - 3s 2ms/step\n",
      "Dev F1-score: 0.9614\n",
      "Epoch 2/100\n",
      "1606/1606 [==============================] - 3s 2ms/step\n",
      "Dev F1-score: 0.9627\n",
      "Epoch 3/100\n",
      "1606/1606 [==============================] - 3s 2ms/step\n",
      "Dev F1-score: 0.9641\n",
      "Epoch 4/100\n",
      "1606/1606 [==============================] - 3s 2ms/step\n",
      "Dev F1-score: 0.9650\n",
      "Epoch 5/100\n",
      "1606/1606 [==============================] - 3s 2ms/step\n",
      "Dev F1-score: 0.9642\n",
      "F1-score on development set is not increasing. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_f1 = 0.0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # Shuffle training data for each epoch (if necessary)\n",
    "    np.random.seed(42)\n",
    "#     shuffle_indices = np.random.permutation(len(train_word_indices))\n",
    "#     train_word_indices = [train_word_indices[i] for i in shuffle_indices]\n",
    "#     train_labels_encoded = [train_labels_encoded[i] for i in shuffle_indices]\n",
    "\n",
    "#     shuffle_indices = np.random.permutation(len(train_words))\n",
    "#     train_word_indices = np.array(train_words)[shuffle_indices]\n",
    "#     train_labels_encoded = train_labels_encoded[shuffle_indices]\n",
    "    shuffle_indices = np.random.permutation(len(train_word_indices))\n",
    "    train_word_indices = np.array(train_word_indices)[shuffle_indices]\n",
    "    train_labels_encoded = train_labels_encoded[shuffle_indices]\n",
    "\n",
    "    # Mini-batch training\n",
    "    for start in range(0, len(train_word_indices), batch_size):\n",
    "        end = min(start + batch_size, len(train_word_indices))\n",
    "\n",
    "        # Convert word indices and labels to numerical format\n",
    "#         X_batch = train_word_indices[start:end]\n",
    "#         y_batch = to_categorical(train_labels_encoded[start:end], num_labels)\n",
    "\n",
    "#         X_batch = np.array(train_words[start:end])\n",
    "#         y_batch = train_labels_encoded[start:end]\n",
    "        \n",
    "        X_batch = train_word_indices[start:end]\n",
    "        y_batch = train_labels_encoded[start:end]\n",
    "        \n",
    "        X_batch = np.expand_dims(X_batch, axis=-1)\n",
    "\n",
    "        # Train the model on the mini-batch\n",
    "#         model.train_on_batch(np.array(X_batch), np.array(y_batch))\n",
    "        model.train_on_batch(X_batch, y_batch)\n",
    "\n",
    "    # Evaluate on the development set and calculate F1-score\n",
    "    dev_predictions = model.predict(np.array(dev_word_indices))\n",
    "#     dev_predictions = model.predict(np.array(dev_words))\n",
    "    dev_predictions = np.argmax(dev_predictions, axis=-1)\n",
    "    dev_f1 = f1_score(dev_labels_encoded, dev_predictions, average='micro')\n",
    "\n",
    "    print(f\"Dev F1-score: {dev_f1:.4f}\")\n",
    "\n",
    "    # Check for early stopping based on F1-score\n",
    "    if dev_f1 > best_f1:\n",
    "        best_f1 = dev_f1\n",
    "    else:\n",
    "        print(\"F1-score on development set is not increasing. Stopping training.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452/1452 [==============================] - 3s 2ms/step\n",
      "Accuracy: 0.9517\n",
      "Precision: 0.9517\n",
      "Recall: 0.9517\n",
      "F1-score: 0.9517\n"
     ]
    }
   ],
   "source": [
    "# Testing on test_df\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "test_word_indices = [google_news_vectors.key_to_index[word] if word in google_news_vectors.key_to_index else 0 for word in test_words]\n",
    "\n",
    "test_predictions = model.predict(np.array(test_word_indices))\n",
    "test_predictions = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "accuracy = accuracy_score(test_labels_encoded, test_predictions)\n",
    "precision = precision_score(test_labels_encoded, test_predictions, average='micro')\n",
    "recall = recall_score(test_labels_encoded, test_predictions, average='micro')\n",
    "f1 = f1_score(test_labels_encoded, test_predictions, average='micro')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "nlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
