{"cells":[{"cell_type":"code","execution_count":null,"id":"e33cd4cd","metadata":{"id":"e33cd4cd","outputId":"57ea3eda-5f50-451f-bcbf-468a8b62e682"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label-coarse</th>\n","      <th>label-fine</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>How did serfdom develop in and then leave Russ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>What films featured the character Popeye Doyle ?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>How can I find a list of celebrities ' real na...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>What fowl grabs the spotlight after the Chines...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>What is the full form of .com ?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label-coarse  label-fine                                               text\n","0             0           0  How did serfdom develop in and then leave Russ...\n","1             1           1   What films featured the character Popeye Doyle ?\n","2             0           0  How can I find a list of celebrities ' real na...\n","3             1           2  What fowl grabs the spotlight after the Chines...\n","4             2           3                    What is the full form of .com ?"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Read the CSV file\n","trec_data = pd.read_csv(\"train.csv\")\n","\n","# View the first 5 rows\n","trec_data.head()"]},{"cell_type":"code","execution_count":null,"id":"8ee55501","metadata":{"id":"8ee55501","outputId":"366ef123-4b98-494d-db58-a15cb6ccdff2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[4, 2, 0, 3]\n"]}],"source":["import random\n","random_4_labels = random.sample(range(5), 4)\n","print(random_4_labels)"]},{"cell_type":"code","execution_count":null,"id":"463138b3","metadata":{"id":"463138b3","outputId":"e173e8e0-047a-4cdf-90a0-43cb0526e77c"},"outputs":[{"data":{"text/plain":["array([0, 'OTHERS', 2, 3, 4], dtype=object)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["trec_updated_labels_df = trec_data\n","\n","def replace_label_coarse(value):\n","    if value not in random_4_labels:\n","        return 'OTHERS' #I use integer 10 to represent 'OTHERS'\n","    return value\n","\n","trec_updated_labels_df['label-coarse'] = trec_updated_labels_df['label-coarse'].apply(replace_label_coarse)\n","trec_updated_labels_df['label-coarse'].unique()"]},{"cell_type":"code","execution_count":null,"id":"377c60cf","metadata":{"id":"377c60cf","outputId":"869510fd-0dcd-47df-af67-943b337e2ddc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label-coarse</th>\n","      <th>label-fine</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>How did serfdom develop in and then leave Russ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>OTHERS</td>\n","      <td>1</td>\n","      <td>What films featured the character Popeye Doyle ?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>How can I find a list of celebrities ' real na...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>OTHERS</td>\n","      <td>2</td>\n","      <td>What fowl grabs the spotlight after the Chines...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>What is the full form of .com ?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  label-coarse  label-fine                                               text\n","0            0           0  How did serfdom develop in and then leave Russ...\n","1       OTHERS           1   What films featured the character Popeye Doyle ?\n","2            0           0  How can I find a list of celebrities ' real na...\n","3       OTHERS           2  What fowl grabs the spotlight after the Chines...\n","4            2           3                    What is the full form of .com ?"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["trec_updated_labels_df.head()"]},{"cell_type":"code","execution_count":null,"id":"541a4c77","metadata":{"id":"541a4c77","outputId":"c8af431e-6026-4a65-8f4e-4d64d44995c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 'OTHERS' 2 3 4]\n","5\n"]}],"source":["classes = trec_updated_labels_df['label-coarse'].unique()\n","print(classes)\n","num_classes = len(classes)\n","print(num_classes)"]},{"cell_type":"code","execution_count":null,"id":"f8cf9f90","metadata":{"id":"f8cf9f90","outputId":"24f61345-c558-4c46-dfb0-a8c04ab6b76d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Label mapping:  {0: 0, 'OTHERS': 1, 2: 2, 3: 3, 4: 4}\n"]}],"source":["di = {}\n","idx = 0\n","for c in classes:\n","    di[c] = idx\n","    idx+=1\n","\n","trec_updated_labels_df.replace({\"label-coarse\": di}, inplace=True)\n","trec_updated_labels_df['label-coarse'].unique()\n","print(\"Label mapping: \", di)"]},{"cell_type":"code","execution_count":null,"id":"b8fbaf82","metadata":{"id":"b8fbaf82"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train, dev = train_test_split(trec_updated_labels_df, test_size=(500/trec_data.shape[0]))"]},{"cell_type":"code","execution_count":null,"id":"ad671a7e","metadata":{"id":"ad671a7e","outputId":"42b49802-d4c8-4dca-ff62-d0f84210a502"},"outputs":[{"data":{"text/plain":["(4952, 3)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train.shape"]},{"cell_type":"code","execution_count":null,"id":"91c1b1b8","metadata":{"id":"91c1b1b8","outputId":"7fd5027f-098d-423e-d211-0a30174c57e0"},"outputs":[{"data":{"text/plain":["(500, 3)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dev.shape"]},{"cell_type":"code","execution_count":null,"id":"9e9f3e56","metadata":{"id":"9e9f3e56","outputId":"25edf342-dfbe-48fb-dbc1-4f8341e4d16f"},"outputs":[{"data":{"text/plain":["array([4, 1, 3, 0, 2])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["test = pd.read_csv(\"test.csv\")\n","test['label-coarse'] = test['label-coarse'].apply(replace_label_coarse)\n","test.replace({\"label-coarse\": di}, inplace=True)\n","test['label-coarse'].unique()"]},{"cell_type":"code","execution_count":null,"id":"c02b9137","metadata":{"id":"c02b9137","outputId":"866165d8-612e-4cfe-cdb5-01b557ba8376"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label-coarse</th>\n","      <th>label-fine</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>How far is it from Denver to Aspen ?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>What county is Modesto , California in ?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>12</td>\n","      <td>Who was Galileo ?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>What is an atom ?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>When did Hawaii become a state ?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label-coarse  label-fine                                      text\n","0             4          40      How far is it from Denver to Aspen ?\n","1             1          21  What county is Modesto , California in ?\n","2             3          12                         Who was Galileo ?\n","3             0           7                         What is an atom ?\n","4             4           8          When did Hawaii become a state ?"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["test.head()"]},{"cell_type":"code","execution_count":null,"id":"bf8ffe1a","metadata":{"id":"bf8ffe1a","outputId":"1257e757-36eb-4897-ce95-b8ed7a073bad"},"outputs":[{"data":{"text/plain":["(500, 3)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test.shape"]},{"cell_type":"code","execution_count":null,"id":"60ed3537","metadata":{"id":"60ed3537","outputId":"585443f7-48b7-4f2a-aeb0-5f1bb8f033da"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-10-17 04:57:52.891623: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-10-17 04:57:52.913292: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-17 04:57:53.003568: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-10-17 04:57:53.003596: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-10-17 04:57:53.004213: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-10-17 04:57:53.053134: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-10-17 04:57:53.054153: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-10-17 04:57:53.575886: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from gensim.models import Word2Vec\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, GlobalAveragePooling1D, Dense\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from gensim.models import Word2Vec\n","import gensim.downloader\n","\n","# Load and preprocess the TREC dataset\n","# (Assuming you have loaded the dataset into train_df, dev_df, and test_df)\n","\n","# Load the pretrained Word2Vec model\n","word2vec_model = gensim.downloader.load('word2vec-google-news-300')"]},{"cell_type":"code","execution_count":null,"id":"6b0f0fe2","metadata":{"id":"6b0f0fe2"},"outputs":[],"source":["# Tokenize and create sequences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train['text'])\n","X_train = tokenizer.texts_to_sequences(train['text'])\n","X_dev = tokenizer.texts_to_sequences(dev['text'])\n","X_test = tokenizer.texts_to_sequences(test['text'])"]},{"cell_type":"code","execution_count":null,"id":"88471308","metadata":{"id":"88471308"},"outputs":[],"source":["# Pad sequences\n","max_seq_length = 200  # You can adjust this\n","X_train = pad_sequences(X_train, maxlen=max_seq_length)\n","X_dev = pad_sequences(X_dev, maxlen=max_seq_length)\n","X_test = pad_sequences(X_test, maxlen=max_seq_length)\n","\n","# Load labels and convert to one-hot encoding\n","num_classes = 5\n","y_train = to_categorical(train['label-coarse'], num_classes=num_classes)\n","y_dev = to_categorical(dev['label-coarse'], num_classes=num_classes)\n","y_test = to_categorical(test['label-coarse'], num_classes=num_classes)"]},{"cell_type":"markdown","id":"5900b49a","metadata":{"id":"5900b49a"},"source":["# Exp 1: LSTM with Global Average Pooling"]},{"cell_type":"code","execution_count":null,"id":"f7dc46ff","metadata":{"id":"f7dc46ff","outputId":"354644e6-ef93-4a33-c860-ee7e8509e455"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-10-17 03:26:17.098097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-10-17 03:26:17.153222: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]}],"source":["# Create the LSTM model\n","model = Sequential()"]},{"cell_type":"code","execution_count":null,"id":"dd0d759e","metadata":{"id":"dd0d759e"},"outputs":[],"source":["# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)"]},{"cell_type":"code","execution_count":null,"id":"f7412e60","metadata":{"id":"f7412e60"},"outputs":[],"source":["# Add an LSTM layer\n","model.add(LSTM(100, return_sequences=True))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(GlobalAveragePooling1D())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"bf43bcf7","metadata":{"id":"bf43bcf7","outputId":"41aed72d-6720-41cf-9089-105952e762a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 10s 58ms/step - loss: 1.4031 - accuracy: 0.3760 - val_loss: 1.3414 - val_accuracy: 0.3760\n","Epoch 2/100\n","  1/155 [..............................] - ETA: 8s - loss: 1.3510 - accuracy: 0.4375"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 9s 56ms/step - loss: 1.2260 - accuracy: 0.4707 - val_loss: 1.1702 - val_accuracy: 0.4920\n","Epoch 3/100\n","155/155 [==============================] - 9s 56ms/step - loss: 1.0709 - accuracy: 0.6254 - val_loss: 1.0717 - val_accuracy: 0.6140\n","Epoch 4/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.9598 - accuracy: 0.7151 - val_loss: 0.9923 - val_accuracy: 0.6340\n","Epoch 5/100\n","155/155 [==============================] - 9s 57ms/step - loss: 0.8746 - accuracy: 0.7510 - val_loss: 0.9169 - val_accuracy: 0.7380\n","Epoch 6/100\n","155/155 [==============================] - 9s 55ms/step - loss: 0.8062 - accuracy: 0.7851 - val_loss: 0.8654 - val_accuracy: 0.7700\n","Epoch 7/100\n","155/155 [==============================] - 9s 57ms/step - loss: 0.7435 - accuracy: 0.8067 - val_loss: 0.8204 - val_accuracy: 0.8000\n","Epoch 8/100\n","155/155 [==============================] - 9s 55ms/step - loss: 0.6957 - accuracy: 0.8277 - val_loss: 0.7969 - val_accuracy: 0.7900\n"]}],"source":["# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"0c098ed1","metadata":{"id":"0c098ed1","outputId":"379ac8bf-531e-4bb7-f4b6-32ffcdffc191"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 25ms/step\n","Test Accuracy: 0.7980\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"44451fd0","metadata":{"id":"44451fd0","outputId":"cdc194a5-3055-4ab9-c91c-a19b12c196d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 200, 300)          2397300   \n","                                                                 \n"," lstm (LSTM)                 (None, 200, 100)          160400    \n","                                                                 \n"," global_average_pooling1d (  (None, 100)               0         \n"," GlobalAveragePooling1D)                                         \n","                                                                 \n"," dense (Dense)               (None, 5)                 505       \n","                                                                 \n","=================================================================\n","Total params: 2558205 (9.76 MB)\n","Trainable params: 160905 (628.54 KB)\n","Non-trainable params: 2397300 (9.14 MB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"d25b380e","metadata":{"id":"d25b380e"},"source":["# Exp 2: LSTM with Weighted Pooling"]},{"cell_type":"code","execution_count":null,"id":"10416d21","metadata":{"id":"10416d21"},"outputs":[],"source":["# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(LSTM(100, return_sequences=True))"]},{"cell_type":"code","execution_count":null,"id":"a69db592","metadata":{"id":"a69db592"},"outputs":[],"source":["# Try using weighted pooling\n","from keras.layers import Layer\n","import tensorflow as tf\n","\n","class WeightedPooling(Layer):\n","    def __init__(self, **kwargs):\n","        super(WeightedPooling, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.custom_weights = self.add_weight(name='custom_weights', shape=(input_shape[-1],), initializer='ones', trainable=True)\n","        super(WeightedPooling, self).build(input_shape)\n","\n","    def call(self, x):\n","        weighted_x = x * self.custom_weights\n","        return tf.reduce_sum(weighted_x, axis=1)\n","\n","# Use the custom WeightedPooling layer in your model\n","model.add(WeightedPooling())"]},{"cell_type":"code","execution_count":null,"id":"15ff1aa1","metadata":{"id":"15ff1aa1","outputId":"1a7a618f-85dc-43a4-b495-77b6df5f6a56"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 10s 59ms/step - loss: 0.8946 - accuracy: 0.6460 - val_loss: 0.6865 - val_accuracy: 0.7100\n","Epoch 2/100\n","  3/155 [..............................] - ETA: 7s - loss: 0.8825 - accuracy: 0.6979 "]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 9s 55ms/step - loss: 0.5572 - accuracy: 0.7971 - val_loss: 0.5602 - val_accuracy: 0.7860\n","Epoch 3/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.4306 - accuracy: 0.8391 - val_loss: 0.4545 - val_accuracy: 0.8460\n","Epoch 4/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.3518 - accuracy: 0.8738 - val_loss: 0.4085 - val_accuracy: 0.8600\n","Epoch 5/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.3038 - accuracy: 0.8891 - val_loss: 0.3966 - val_accuracy: 0.8600\n"]}],"source":["# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"a38704fe","metadata":{"id":"a38704fe","outputId":"1fb452bf-4832-4ac9-fa0a-4b5bb1f875bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 23ms/step\n","Test Accuracy: 0.8360\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"f76d1b60","metadata":{"id":"f76d1b60"},"source":["# Exp 3: LSTM with GAP. Change loss function to categorical crossentropy with label smoothing"]},{"cell_type":"code","execution_count":null,"id":"f3d34212","metadata":{"id":"f3d34212"},"outputs":[],"source":["# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(LSTM(100, return_sequences=True))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(GlobalAveragePooling1D())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"id":"bc7afa81","metadata":{"id":"bc7afa81"},"outputs":[],"source":["def categorical_crossentropy_with_label_smoothing(y_true, y_pred, label_smoothing=0.1):\n","    y_true_smoothed = (1 - label_smoothing) * y_true + label_smoothing / num_classes  # Assuming num_classes is defined\n","    return tf.keras.losses.categorical_crossentropy(y_true_smoothed, y_pred)\n","\n","# Compile your model with the custom loss function\n","model.compile(loss=categorical_crossentropy_with_label_smoothing, optimizer=Adam(), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"933d6769","metadata":{"id":"933d6769","outputId":"afa59c09-5221-4a8a-86bf-1374c942382f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 10s 58ms/step - loss: 1.4477 - accuracy: 0.3766 - val_loss: 1.3944 - val_accuracy: 0.3820\n","Epoch 2/100\n","  3/155 [..............................] - ETA: 7s - loss: 1.3915 - accuracy: 0.3646"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 9s 57ms/step - loss: 1.3076 - accuracy: 0.4814 - val_loss: 1.2668 - val_accuracy: 0.6020\n","Epoch 3/100\n","155/155 [==============================] - 9s 57ms/step - loss: 1.1772 - accuracy: 0.6468 - val_loss: 1.1653 - val_accuracy: 0.7040\n","Epoch 4/100\n","155/155 [==============================] - 9s 56ms/step - loss: 1.0842 - accuracy: 0.7155 - val_loss: 1.1031 - val_accuracy: 0.7000\n"]}],"source":["# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"53dcebf8","metadata":{"id":"53dcebf8","outputId":"cf9a719c-c0bd-49f8-afc7-817de3b5f66c"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 50ms/step\n","Test Accuracy: 0.4940\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"d6386f25","metadata":{"id":"d6386f25"},"source":["# Exp 4: LSTM with Weighted Pooling. Change loss function to categorical crossentropy with label smoothing"]},{"cell_type":"code","execution_count":null,"id":"57e05db7","metadata":{"id":"57e05db7","outputId":"a2edaa82-8cf5-4475-ee8a-899a4c9acd40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 10s 57ms/step - loss: 1.0639 - accuracy: 0.6488 - val_loss: 0.9000 - val_accuracy: 0.7700\n","Epoch 2/100\n","  2/155 [..............................] - ETA: 9s - loss: 0.7414 - accuracy: 0.8281"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 9s 56ms/step - loss: 0.8465 - accuracy: 0.7849 - val_loss: 0.8266 - val_accuracy: 0.7940\n","Epoch 3/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.7478 - accuracy: 0.8389 - val_loss: 0.7531 - val_accuracy: 0.8380\n","Epoch 4/100\n","155/155 [==============================] - 8s 54ms/step - loss: 0.6762 - accuracy: 0.8760 - val_loss: 0.7429 - val_accuracy: 0.8240\n"]}],"source":["# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(LSTM(100, return_sequences=True))\n","\n","from keras.layers import Layer\n","import tensorflow as tf\n","\n","class WeightedPooling(Layer):\n","    def __init__(self, **kwargs):\n","        super(WeightedPooling, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.custom_weights = self.add_weight(name='custom_weights', shape=(input_shape[-1],), initializer='ones', trainable=True)\n","        super(WeightedPooling, self).build(input_shape)\n","\n","    def call(self, x):\n","        weighted_x = x * self.custom_weights\n","        return tf.reduce_sum(weighted_x, axis=1)\n","\n","# Use the custom WeightedPooling layer in your model\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","def categorical_crossentropy_with_label_smoothing(y_true, y_pred, label_smoothing=0.1):\n","    y_true_smoothed = (1 - label_smoothing) * y_true + label_smoothing / num_classes  # Assuming num_classes is defined\n","    return tf.keras.losses.categorical_crossentropy(y_true_smoothed, y_pred)\n","\n","# Compile your model with the custom loss function\n","model.compile(loss=categorical_crossentropy_with_label_smoothing, optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"4d161bb9","metadata":{"id":"4d161bb9","outputId":"c2c66ff1-b8b9-401b-fb3b-c1c3ce4dda8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 25ms/step\n","Test Accuracy: 0.8640\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"fa59d02e","metadata":{"id":"fa59d02e"},"source":["# Exp 5: LSTM with Weighted Pooling. Change loss function to focal loss"]},{"cell_type":"code","execution_count":null,"id":"b86034ad","metadata":{"id":"b86034ad","outputId":"e1fc7523-2424-4218-988f-0c88057b4c60"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 10s 58ms/step - loss: 0.0290 - accuracy: 0.6349 - val_loss: 0.0242 - val_accuracy: 0.6520\n","Epoch 2/100\n","  3/155 [..............................] - ETA: 8s - loss: 0.0189 - accuracy: 0.7083"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 9s 56ms/step - loss: 0.0177 - accuracy: 0.7748 - val_loss: 0.0200 - val_accuracy: 0.7460\n","Epoch 3/100\n","155/155 [==============================] - 9s 56ms/step - loss: 0.0139 - accuracy: 0.8273 - val_loss: 0.0146 - val_accuracy: 0.8280\n","Epoch 4/100\n","155/155 [==============================] - 9s 55ms/step - loss: 0.0112 - accuracy: 0.8526 - val_loss: 0.0150 - val_accuracy: 0.7940\n"]}],"source":["# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(LSTM(100, return_sequences=True))\n","\n","from keras.layers import Layer\n","import tensorflow as tf\n","\n","class WeightedPooling(Layer):\n","    def __init__(self, **kwargs):\n","        super(WeightedPooling, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.custom_weights = self.add_weight(name='custom_weights', shape=(input_shape[-1],), initializer='ones', trainable=True)\n","        super(WeightedPooling, self).build(input_shape)\n","\n","    def call(self, x):\n","        weighted_x = x * self.custom_weights\n","        return tf.reduce_sum(weighted_x, axis=1)\n","\n","# Use the custom WeightedPooling layer in your model\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","def focal_loss(gamma=2.0, alpha=0.25):\n","    def focal_loss_fixed(y_true, y_pred):\n","        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n","        focal_loss = -alpha * (1 - pt)**gamma * tf.math.log(pt)\n","        return focal_loss\n","\n","    return focal_loss_fixed\n","\n","# Compile your model with the custom loss function\n","model.compile(loss=focal_loss(), optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"63f16e06","metadata":{"id":"63f16e06","outputId":"43d52d89-5736-48bd-9da5-2459db0e36e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 24ms/step\n","Test Accuracy: 0.8180\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"db64eb77","metadata":{"id":"db64eb77"},"source":["# Exp 6: Bidirectional LSTM with GAP."]},{"cell_type":"code","execution_count":null,"id":"598edb76","metadata":{"id":"598edb76","outputId":"2c175829-8169-4226-83d3-ecbff5b306e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 21s 119ms/step - loss: 1.2168 - accuracy: 0.4885 - val_loss: 0.9758 - val_accuracy: 0.6200\n","Epoch 2/100\n","  1/155 [..............................] - ETA: 21s - loss: 0.8929 - accuracy: 0.5938"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 18s 117ms/step - loss: 0.9366 - accuracy: 0.6440 - val_loss: 0.9713 - val_accuracy: 0.5580\n"]}],"source":["from keras.layers import Bidirectional\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add another LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(GlobalAveragePooling1D())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"fc77ac7e","metadata":{"id":"fc77ac7e","outputId":"01ea5e42-08a8-4c1b-eed9-39718490ad11"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 2s 44ms/step\n","Test Accuracy: 0.6200\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"c19d8d15","metadata":{"id":"c19d8d15"},"source":["# Exp 7: Bidirectional LSTM with Weighted Pooling"]},{"cell_type":"code","execution_count":null,"id":"3e475791","metadata":{"id":"3e475791","outputId":"adfb8b01-8ac1-408f-cf40-b130edfd3679"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 24s 127ms/step - loss: 0.9504 - accuracy: 0.6393 - val_loss: 0.7915 - val_accuracy: 0.6860\n","Epoch 2/100\n","  1/155 [..............................] - ETA: 19s - loss: 0.5052 - accuracy: 0.8438"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 19s 125ms/step - loss: 0.5784 - accuracy: 0.7904 - val_loss: 0.4921 - val_accuracy: 0.8100\n","Epoch 3/100\n","155/155 [==============================] - 19s 123ms/step - loss: 0.4320 - accuracy: 0.8471 - val_loss: 0.4487 - val_accuracy: 0.8120\n","Epoch 4/100\n","155/155 [==============================] - 19s 124ms/step - loss: 0.3470 - accuracy: 0.8750 - val_loss: 0.5850 - val_accuracy: 0.7700\n"]}],"source":["from keras.layers import Bidirectional\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add another LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"dba8da40","metadata":{"id":"dba8da40","outputId":"8d683625-c979-4431-a404-623f44596023"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 46ms/step\n","Test Accuracy: 0.8380\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"cab7e726","metadata":{"id":"cab7e726"},"source":["# Exp 8: Bidirectional LSTM with Weighted Pooling. Change loss function to categorical crossentropy with label smoothing"]},{"cell_type":"code","execution_count":null,"id":"1b2fbe23","metadata":{"id":"1b2fbe23","outputId":"a46a64cd-979f-4ce2-c451-f9196a84412a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 14s 69ms/step - loss: 1.0973 - accuracy: 0.6468 - val_loss: 0.8646 - val_accuracy: 0.7660\n","Epoch 2/100\n","  2/155 [..............................] - ETA: 12s - loss: 0.9564 - accuracy: 0.7500"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 10s 66ms/step - loss: 0.8385 - accuracy: 0.7880 - val_loss: 0.8325 - val_accuracy: 0.8040\n","Epoch 3/100\n","155/155 [==============================] - 11s 71ms/step - loss: 0.7406 - accuracy: 0.8461 - val_loss: 0.7545 - val_accuracy: 0.8480\n","Epoch 4/100\n","155/155 [==============================] - 11s 70ms/step - loss: 0.6919 - accuracy: 0.8722 - val_loss: 0.7701 - val_accuracy: 0.8080\n"]}],"source":["from keras.layers import Bidirectional\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","def categorical_crossentropy_with_label_smoothing(y_true, y_pred, label_smoothing=0.1):\n","    y_true_smoothed = (1 - label_smoothing) * y_true + label_smoothing / num_classes  # Assuming num_classes is defined\n","    return tf.keras.losses.categorical_crossentropy(y_true_smoothed, y_pred)\n","\n","# Compile your model with the custom loss function\n","model.compile(loss=categorical_crossentropy_with_label_smoothing, optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"1511ced4","metadata":{"id":"1511ced4","outputId":"df64b553-daa1-485e-fc2d-339462861669"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 24ms/step\n","Test Accuracy: 0.8580\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"63b7c587","metadata":{"id":"63b7c587"},"source":["# Exp 9: Bidirectional LSTM with Weighted Pooling and Self-Attention"]},{"cell_type":"code","execution_count":null,"id":"a84c1e38","metadata":{"id":"a84c1e38","outputId":"c311cec6-6199-477d-ad11-2b8434f20699"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 26s 137ms/step - loss: 0.9569 - accuracy: 0.6387 - val_loss: 0.6443 - val_accuracy: 0.7560\n","Epoch 2/100\n","  1/155 [..............................] - ETA: 19s - loss: 0.4628 - accuracy: 0.8438"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 21s 134ms/step - loss: 0.5634 - accuracy: 0.7944 - val_loss: 0.4906 - val_accuracy: 0.8420\n","Epoch 3/100\n","155/155 [==============================] - 21s 134ms/step - loss: 0.4142 - accuracy: 0.8548 - val_loss: 0.4997 - val_accuracy: 0.8000\n"]}],"source":["from keras.layers import Bidirectional\n","from keras_self_attention import SeqSelfAttention\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add another LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add self-attention mechanism\n","model.add(SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n","\n","# Add Global Average Pooling1D for aggregation\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"d7fe7c64","metadata":{"id":"d7fe7c64","outputId":"8d38c52e-03cf-4652-86bf-543f0f48a323"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 50ms/step\n","Test Accuracy: 0.8620\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"31395dcd","metadata":{"id":"31395dcd"},"source":["# Exp 10: Bidirectional LSTM with Weighted Pooling and Self-Attention. Change loss function to categorical crossentropy with label smoothing"]},{"cell_type":"code","execution_count":null,"id":"f1c7176f","metadata":{"id":"f1c7176f","outputId":"4ca491ff-5569-4cbd-82d6-a8394db19590"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-10-17 05:01:38.414792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-10-17 05:01:38.552694: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n","Skipping registering GPU devices...\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","155/155 [==============================] - 25s 127ms/step - loss: 1.1220 - accuracy: 0.6315 - val_loss: 0.8748 - val_accuracy: 0.7620\n","Epoch 2/100\n","  1/155 [..............................] - ETA: 19s - loss: 0.7805 - accuracy: 0.8750"]},{"name":"stderr","output_type":"stream","text":["/home/grace/anaconda3/envs/nlp2/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["155/155 [==============================] - 20s 126ms/step - loss: 0.8042 - accuracy: 0.8122 - val_loss: 0.9848 - val_accuracy: 0.7120\n"]}],"source":["from keras.layers import Bidirectional\n","from keras_self_attention import SeqSelfAttention\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer with pretrained word embeddings\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocab_size, 300))  # Adjust embedding dimension\n","for word, i in tokenizer.word_index.items():\n","    if word in word2vec_model:\n","        embedding_matrix[i] = word2vec_model[word]\n","embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_seq_length, trainable=False)\n","model.add(embedding_layer)\n","\n","# Add an LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add another LSTM layer\n","model.add(Bidirectional(LSTM(100, return_sequences=True)))\n","\n","# Add self-attention mechanism\n","model.add(SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n","\n","# Add weighted pooling\n","from keras.layers import Layer\n","import tensorflow as tf\n","\n","class WeightedPooling(Layer):\n","    def __init__(self, **kwargs):\n","        super(WeightedPooling, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.custom_weights = self.add_weight(name='custom_weights', shape=(input_shape[-1],), initializer='ones', trainable=True)\n","        super(WeightedPooling, self).build(input_shape)\n","\n","    def call(self, x):\n","        weighted_x = x * self.custom_weights\n","        return tf.reduce_sum(weighted_x, axis=1)\n","\n","model.add(WeightedPooling())\n","\n","# Add the output layer\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile your model with the custom loss function\n","def categorical_crossentropy_with_label_smoothing(y_true, y_pred, label_smoothing=0.1):\n","    y_true_smoothed = (1 - label_smoothing) * y_true + label_smoothing / num_classes  # Assuming num_classes is defined\n","    return tf.keras.losses.categorical_crossentropy(y_true_smoothed, y_pred)\n","\n","model.compile(loss=categorical_crossentropy_with_label_smoothing, optimizer=Adam(), metrics=['accuracy'])\n","\n","# Training with early stopping\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=1, restore_best_weights=True)\n","model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n","\n","history = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=100, batch_size=32, callbacks=[early_stopping, model_checkpoint])"]},{"cell_type":"code","execution_count":null,"id":"fe3e82ee","metadata":{"id":"fe3e82ee","outputId":"70f4d3fd-deb5-407b-a6ec-7257d2edbce0"},"outputs":[{"name":"stdout","output_type":"stream","text":["16/16 [==============================] - 1s 49ms/step\n","Test Accuracy: 0.7840\n"]}],"source":["# Evaluate on the test set\n","test_predictions = model.predict(X_test)\n","test_predictions = np.argmax(test_predictions, axis=-1)\n","test_accuracy = accuracy_score(np.argmax(y_test, axis=-1), test_predictions)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")"]},{"cell_type":"markdown","id":"ebae26d9","metadata":{"id":"ebae26d9"},"source":["# Summary\n","\n","### LSTM with Global Average Pooling, Categorical Cross Entropy Loss\n","Val accuracy: 0.800\n","Test Accuracy: 0.7980\n","\n","### LSTM with Weighted Pooling, Categorical Cross Entropy Loss\n","Val accuracy: 0.860\n","Test Accuracy: 0.836\n","\n","### LSTM with Global Average Pooling, Categorical Cross Entropy Loss with Label Smoothing\n","Val accuracy: 0.7040\n","Test Accuracy: 0.4940\n","\n","### LSTM with Weighted Pooling, Categorical Cross Entropy Loss with Label Smoothing\n","Val accuracy: 0.8380\n","Test Accuracy: 0.8640\n","\n","### LSTM with Weighted Pooling, Focal Loss\n","Val accuracy: 0.8280\n","Test Accuracy: 0.8180\n","\n","### Bidirectional LSTM with Global Average Pooling, Categorical Cross Entropy Loss\n","Val accuracy: 0.6200\n","Test Accuracy: 0.6200\n","\n","### Bidirectional LSTM with Weighted Pooling, Categorical Cross Entropy Loss\n","Val accuracy: 0.8120\n","Test Accuracy: 0.8380\n","\n","### Bidirectional LSTM with Weighted Pooling, Categorical Cross Entropy Loss with Label Smoothing\n","Val accuracy: 0.8480\n","Test Accuracy: 0.8580\n","\n","### Bidirectional LSTM with Weighted Pooling and Self-Attention, Categorical Cross Entropy Loss\n","Val accuracy: 0.8420\n","Test Accuracy: 0.8620\n","\n","### Bidirectional LSTM with Weighted Pooling and Self-Attention, Categorical Cross Entropy Loss with Label Smoothing\n","Val accuracy: 0.7620\n","Test Accuracy: 0.7840"]},{"cell_type":"code","execution_count":null,"id":"cfe6757d","metadata":{"id":"cfe6757d"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"nlp2","language":"python","name":"nlp2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}